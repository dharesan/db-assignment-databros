{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m9yXqV3LigUA"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dharesan/db-assignment-databros/blob/main/assignment_toy_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fjpkeS-nYP"
      },
      "source": [
        "# Lab 11 Spark\n",
        "\n",
        "Author: ISTD, SUTD\n",
        "\n",
        "Title: Lab 11, Spark part 1\n",
        "\n",
        "Date: March 5, 2025\n",
        "\n",
        "## Learning outcome\n",
        "\n",
        "\n",
        "By the end of this lesson, you are able to\n",
        "\n",
        "* Submit PySpark jobs to a Spark cluster\n",
        "* Paralelize data processing using PySpark\n",
        "\n",
        "\n",
        "You can either execute this lab directly on the aws cluster with HDFS file system, or you can install PySpark in Google Colab and load the files locally. The main difference in coding is that we do not load the context from the HDFS filesystem, but instead just load a local file. Other than than that, all PySpark commands are the same.\n",
        "\n",
        "To run this lab, you can make a copy of this notebook or `File -> Open in Playground Mode`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "## Installing PySpark in Google Colab\n",
        "\n",
        "To install PySpark in Google Collab, execute the below cell. This will download Spark and install all necessary libraries for this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxv7w_2y2bb9"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wordcount Example\n",
        "\n",
        "Let us first download the necessary data file. We can find it at `https://raw.githubusercontent.com/istd50043-2023-spring/cohort_problems/main/cc11/ex1/data.csv`.\n",
        "\n",
        "Colab lets us execute unix commands, as long as we prepend them with `!`. So let's download the file and move it into a new folder called `input`. While we are at it, let's create a folder called `output` as well."
      ],
      "metadata": {
        "id": "r_DCGGK3-E6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc10/data/TheCompleteSherlockHolmes.txt\n",
        "!mkdir input\n",
        "!mv TheCompleteSherlockHolmes.txt input/\n",
        "!mkdir output"
      ],
      "metadata": {
        "id": "z0LtAmEBoSDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## testing commit message"
      ],
      "metadata": {
        "id": "1nhSf0Cv2nC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check that the data.csv file downloaded by uncollapsing the left panel and checking the folder contents.\n",
        "\n",
        "Now we are ready to write our PySpark code. The goal is to write a simple wordcounter:"
      ],
      "metadata": {
        "id": "7ANacU_Go4bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# sc.stop() # uncomment this during debugging to restart your context in case execution stopped mid-way this cell.\n",
        "\n",
        "conf = SparkConf().setAppName(\"Wordcount Application\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# note that we load the text file directly with a local path instead of providing an hdfs url\n",
        "input_file_name = 'input/TheCompleteSherlockHolmes.txt'\n",
        "text_file = sc.textFile(input_file_name)\n",
        "\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "output_folder = './output/wordcount'\n",
        "counts.saveAsTextFile(output_folder)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "pzse2gw82OFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "Write a PySpark application which takes a (set of) Comma-seperated-value (CSV) file(s) with 2 columns and output a CSV file with first two columns same as the input file, and the third column contains the values obtained by splitting the first column using the second column as delimiter.\n",
        "\n",
        "The input file can be found here: `https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex1/data.csv`.\n",
        "\n",
        "For example, given input from a file:\n",
        "\n",
        "```\n",
        "50000.0#0#0#,#\n",
        "0@1000.0@,@\n",
        "1$,$\n",
        "1000.00^Test_string,^\n",
        "```\n",
        "\n",
        "\n",
        "the program should output\n",
        "\n",
        "```\n",
        "50000.0#0#0#,#,['50000.0', '0', '0']\n",
        "0@1000.0@,@,['0', '1000.0', '']\n",
        "1$,$,['1', '']\n",
        "1000.00^Test_string,^,['1000.00', 'Test_string']\n",
        "```\n",
        "\n",
        "and write it to a file.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZHg9lXG8bwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your answer"
      ],
      "metadata": {
        "id": "F1aLxWrqRyAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "Write PySpark application which aggregates (counts) a (set of) CSV file(s) with 4 columns based on its third column, the destination IP.\n",
        "\n",
        "The input file can be found here: `https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex2/data.csv`\n",
        "\n",
        "Given input\n",
        "\n",
        "```\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "```\n",
        "the program should output\n",
        "\n",
        "```\n",
        " 10.0.0.3.5001,13\n",
        " 10.0.0.2.54880,7\n",
        "```"
      ],
      "metadata": {
        "id": "tIYQgNbA963l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your answer"
      ],
      "metadata": {
        "id": "T-coxW5U9690"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "Given the same input as Exercise 2, write a PySpark application which outputs the following:\n",
        "\n",
        "```\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "```\n",
        "\n",
        "\n",
        "In the event the input is very huge with too many unique destination IP values, can your program scale?\n",
        "\n",
        "\n",
        "The questions were adopted from `https://jaceklaskowski.github.io/spark-workshop/exercises/`\n"
      ],
      "metadata": {
        "id": "0BIgiKCh97D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your answer"
      ],
      "metadata": {
        "id": "IkEQswDg97Iu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}